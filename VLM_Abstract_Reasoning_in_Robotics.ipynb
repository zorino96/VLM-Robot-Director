# Part 1: Installing and Importing Requirements
# This cell installs all necessary libraries for the project.
!pip install pybullet -q
!pip install imageio-ffmpeg -q
!pip install transformers bitsandbytes accelerate -q
print("âœ… All requirements installed successfully.")

import pybullet as p
import pybullet_data
import numpy as np
import imageio
from IPython.display import display, HTML
from base64 import b64encode
from PIL import Image
import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration
from google.colab import userdata
import re

print("Libraries imported successfully.")

# Part 2: Login to Hugging Face
# This cell logs into Hugging Face using a token stored in Colab's secret manager.
# To run this, you must add your Hugging Face token as a secret named 'HF_TOKEN'.
try:
    hf_token = userdata.get('HF_TOKEN')
    from huggingface_hub import login
    login(token=hf_token)
    print("âœ… Successfully logged in to Hugging Face!")
except userdata.SecretNotFoundError:
    print("âš ï¸ Secret 'HF_TOKEN' not found. Please add it to your Colab secrets.")
except Exception as e:
    print(f"An error occurred during login: {e}")

# ---
# # The Grand Challenge: VLM-driven Abstract Reasoning for Robotic Control

# In this notebook, we present a proof-of-concept for a hierarchical robotic control system where a Vision-Language Model (VLM) acts as a high-level "director." The system is tasked with interpreting abstract, common-sense user commands and translating them into appropriate physical actions within a simulated environment.

# ## 1. Architecture Components

# Our architecture consists of three main parts:
# 1.  **The VLM Director (LLaVA-1.5 7B):** The high-level brain. It perceives the visual scene and understands the user's abstract command to choose a logical action.
# 2.  **The Skill Library:** A predefined set of reliable, low-level motor primitives (e.g., `point_at_something`).
# 3.  **The Skill Executor:** A function that executes the chosen skill in the PyBullet simulation.

# ---
# Part 3: Skill Library and Executor Definition

# The Skill Library contains pre-programmed animations.
SKILL_LIBRARY = {
    "point_at_something": [
        {'joint_positions': [0, 0.5, 0, -1.5, 0, 0.5, 0], 'duration': 2.0},
        {'joint_positions': [0, 0, 0, 0, 0, 0, 0], 'duration': 1.5},
    ],
    "do_nothing": [ {'joint_positions': [0, 0, 0, 0, 0, 0, 0], 'duration': 2.0} ]
}

# The Skill Executor translates a skill name into physical actions.
def execute_skill(skill_name: str, robot_id: int, num_joints: int, client: int, frames_list: list):
    skill_to_execute = SKILL_LIBRARY.get(skill_name, SKILL_LIBRARY["do_nothing"])
    for step in skill_to_execute:
        target_positions = step['joint_positions']
        duration = step['duration']
        num_sim_steps = int(duration * 240)
        for i in range(num_sim_steps):
            p.setJointMotorControlArray(robot_id, range(num_joints), p.POSITION_CONTROL, target_positions, physicsClientId=client)
            p.stepSimulation(physicsClientId=client)
            if i % 8 == 0:
                v_mat = p.computeViewMatrixFromYawPitchRoll([1.2, 0, 1.5], 2.8, 90, -35, 0, 2)
                p_mat = p.computeProjectionMatrixFOV(60, 1.0, 0.1, 100.0)
                _, _, rgb, _, _ = p.getCameraImage(480, 480, v_mat, p_mat, renderer=p.ER_BULLET_HARDWARE_OPENGL)
                frames_list.append(np.array(rgb)[:, :, :3])

print("âœ… Skill Library and Executor are defined.")

# ---
# Part 4: VLM Director Class Definition

# This class encapsulates the logic for loading the LLaVA model and using it to make decisions.
class VLMDirector:
    def __init__(self, hf_token: str, device="cuda"):
        self.device, self.token, self.model, self.processor = device, hf_token, None, None
    
    def load_model(self):
        print("Loading 7B Vision Language Model (LLaVA)...")
        model_id = "llava-hf/llava-1.5-7b-hf"
        self.model = LlavaForConditionalGeneration.from_pretrained(model_id, token=self.token, torch_dtype=torch.float16, load_in_4bit=True)
        self.processor = AutoProcessor.from_pretrained(model_id, token=self.token)
        print("âœ… VLM loaded successfully!")
        
    def choose_action(self, image: Image.Image, command: str, scene_description: str) -> str:
        if self.model is None: self.load_model()
        
        # This advanced prompt encourages common-sense reasoning.
        prompt = f"""USER: <image>
You are the brain of a helpful robot. Your task is to understand the user's abstract command and choose the most logical action.
- The scene contains: {scene_description}
- The user's command is: "{command}"
- Your only available action is: 'point_at_something'.

Which object should you point at to fulfill the user's command? Respond with ONLY the name of the object (e.g., "red box", "blue box", "green sphere").
"""
        inputs = self.processor(text=prompt, images=image, return_tensors="pt").to(self.device, torch.float16)
        output = self.model.generate(**inputs, max_new_tokens=15)
        decoded_output = self.processor.decode(output[0], skip_special_tokens=True)
        response = decoded_output.split("ASSISTANT:")[-1].strip().lower()
        print(f"ðŸ§  VLM's Reasoning/Output: '{response}'")
        return response

print("âœ… VLM Director class is defined.")
# ---
# ## 2. The Grand Challenge Experiment

# We now set up the main experiment. The environment contains three functionally distinct objects. The VLM will be given abstract commands and must deduce the correct target object.

# ---
# Part 5: Experiment Execution
print("--- ðŸš€ Starting The Grand Challenge: Abstract Reasoning ---")
director = VLMDirector(hf_token=hf_token)

# Setup simulation environment
if p.isConnected(): p.disconnect()
client = p.connect(p.DIRECT)
p.setAdditionalSearchPath(pybullet_data.getDataPath())
p.loadURDF("plane.urdf", physicsClientId=client)
robot_id = p.loadURDF("kuka_iiwa/model.urdf", [0, 0, 0], useFixedBase=True, physicsClientId=client)
num_joints = p.getNumJoints(robot_id)

# Create the scene with functionally distinct objects
p.createMultiBody(baseVisualShapeIndex=p.createVisualShape(p.GEOM_BOX, halfExtents=[0.1,0.1,0.1], rgbaColor=[1,0,0,1]), basePosition=[0.7, 0.4, 0.1]) # Red Box (Food)
p.createMultiBody(baseVisualShapeIndex=p.createVisualShape(p.GEOM_BOX, halfExtents=[0.1,0.1,0.1], rgbaColor=[0,0,1,1]), basePosition=[0.7, -0.4, 0.1]) # Blue Box (Water)
p.createMultiBody(baseVisualShapeIndex=p.createVisualShape(p.GEOM_SPHERE, radius=0.1, rgbaColor=[0,1,0,1]), basePosition=[1.0, 0, 0.1]) # Green Sphere (Toy)
scene_description = "a red box (representing a food station), a blue box (representing a water station), and a green sphere (representing a toy)."

# Capture the scene image for the VLM
v_mat = p.computeViewMatrixFromYawPitchRoll([1.2, 0, 1.5], 2.8, 90, -35, 0, 2)
p_mat = p.computeProjectionMatrixFOV(60, 1.0, 0.1, 100.0)
_, _, rgb, _, _ = p.getCameraImage(480, 480, v_mat, p_mat, renderer=p.ER_BULLET_HARDWARE_OPENGL)
scene_image = Image.fromarray(np.array(rgb)[:, :, :3])

all_frames = []

# --- Task 1: The Hunger Test ---
command1 = "I am hungry."
print(f"\nCOMMAND 1: {command1}")
chosen_object1 = director.choose_action(scene_image, command1, scene_description)
chosen_skill1 = "point_at_something" if "red" in chosen_object1 else "do_nothing"
execute_skill(chosen_skill1, robot_id, num_joints, client, all_frames)

# --- Task 2: The Playtime Test ---
command2 = "Let's play."
print(f"\nCOMMAND 2: {command2}")
chosen_object2 = director.choose_action(scene_image, command2, scene_description)
chosen_skill2 = "point_at_something" if "green" in chosen_object2 else "do_nothing"
execute_skill(chosen_skill2, robot_id, num_joints, client, all_frames)

p.disconnect(client)
print("\nâœ… Grand Challenge finished.")

# ---
# ## 3. Results and Conclusion

# The following video demonstrates the outcome of the experiment.
# ---
# Part 6: Video Creation and Display
video_filename = 'grand_challenge_demo.mp4'
print(f"ðŸŽ¬ Creating video: {video_filename}")
imageio.mimsave(video_filename, all_frames, fps=30)
mp4 = open(video_filename, 'rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()

display(HTML(f"""
<h2>ðŸŽ¬ The Grand Challenge: Abstract Reasoning ðŸŽ¬</h2>
<p><b>Task 1:</b> "{command1}" -> <b>VLM identified:</b> "{chosen_object1}" -> <b>Executed:</b> "{chosen_skill1}"</p>
<p><b>Task 2:</b> "{command2}" -> <b>VLM identified:</b> "{chosen_object2}" -> <b>Executed:</b> "{chosen_skill2}"</p>
<video width=500 controls loop><source src="{data_url}" type="video/mp4"></video>
"""))
display(HTML("<h3>Scene Presented to the VLM:</h3>"))
display(scene_image)
